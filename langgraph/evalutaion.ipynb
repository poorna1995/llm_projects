{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28a61d9",
   "metadata": {},
   "source": [
    "#### to get the strucred putpt \n",
    "\n",
    "\n",
    "till now ae have use one agent (whichi is chatbot llm). nowe we goana add one more agent which is evaluator agent . this evaluator check of the assistanct answer meets the success cirteri. this create a multi agent loops : \n",
    "worker --> evalutor --> maybe back to worker\n",
    "\n",
    "\n",
    "also : intriduced a structed output ( reposes in fixed json scuema ) to keep evertu prdcite\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26366c",
   "metadata": {},
   "source": [
    "structre foutput\n",
    "1. normally llm reply in free text --> palintest\n",
    "2. wih stuced output, we can define a schema ( usinf pydanitc\n",
    "3. the llms reposen must follow the schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bcb6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import Annotated, Optional, List, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "import requests\n",
    "import os\n",
    "from langchain.agents import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import display, Image\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "745e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback : str = Field(description= \"feedbasck on the assistant reponse\")# why the answer was good or bad\n",
    "    success_criteria_met: bool = Field(description=\"wheather the success cirterual have benn met\")\n",
    "    user_input_needed : bool = Field(description = \"True if more input is needed form the user, or clarification, or assitant is stuck\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd5034cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages : Annotated[List[Any], add_messages]\n",
    "    success_criteria :str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met : bool\n",
    "    user_input_needed : bool\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dec897b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "async_browser = create_async_playwright_browser(headless=False)\n",
    "toolkit = PlayWrightBrowserToolkit(async_browser= async_browser)\n",
    "tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2902e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## assistant \n",
    "\n",
    "assistant_llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "evaluation_llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "\n",
    "assistant_llm_tools = assistant_llm.bind_tools(tools)\n",
    "evaluation_llm_tools = evaluation_llm.with_structured_output(EvaluatorOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1366c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x16786b110>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x16786b4d0>, root_client=<openai.OpenAI object at 0x16786ae90>, root_async_client=<openai.AsyncOpenAI object at 0x16786b250>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'response_format': <class '__main__.EvaluatorOutput'>, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'EvaluatorOutput', 'description': '', 'parameters': {'properties': {'feedback': {'description': 'feedbasck on the assistant reponse', 'type': 'string'}, 'success_criteria_met': {'description': 'wheather the success cirterual have benn met', 'type': 'boolean'}, 'user_input_needed': {'description': 'True if more input is needed form the user, or clarification, or assitant is stuck', 'type': 'boolean'}}, 'required': ['feedback', 'success_criteria_met', 'user_input_needed'], 'type': 'object'}}}}}, config={}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(...), kwargs={}, config={}, config_factories=[], custom_output_type=<class '__main__.EvaluatorOutput'>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_llm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0413532",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The worker node\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massistant\u001b[39m(state: State) -> \u001b[43mDict\u001b[49m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m      4\u001b[39m     system_message = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a helpful assistant that can use tools to complete tasks.\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mYou keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mThis is the success criteria:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33mIf you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve finished, reply with the final answer, and don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt ask a question; simply reply with the answer.\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state.get(\u001b[33m\"\u001b[39m\u001b[33mfeedback_on_work\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "# The worker node\n",
    "\n",
    "def assistant(state: State) -> Dict[str, Any]:\n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "This is the success criteria:\n",
    "{state['success_criteria']}\n",
    "You should reply either with a question for the user about this assignment, or with your final response.\n",
    "If you have a question for the user, you need to reply by clearly stating your question. An example might be:\n",
    "\n",
    "Question: please clarify whether you want a summary or a detailed answer\n",
    "\n",
    "If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.\n",
    "\"\"\"\n",
    "    \n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_message += f\"\"\"\n",
    "Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "Here is the feedback on why this was rejected:\n",
    "{state['feedback_on_work']}\n",
    "With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.\"\"\"\n",
    "    \n",
    "    # Add in the system message\n",
    "\n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "    \n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "    \n",
    "    # Invoke the LLM with tools\n",
    "    response = worker_llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d92ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_router(state: State) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"evaluator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = \"Conversation history:\\n\\n\"\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation += f\"User: {message.content}\\n\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            text = message.content or \"[Tools use]\"\n",
    "            conversation += f\"Assistant: {text}\\n\"\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56809a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "\n",
    "    system_message = \"\"\"You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "Assess the Assistant's last response based on the given criteria. Respond with your feedback, and with your decision on whether the success criteria has been met,\n",
    "and whether more input is needed from the user.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "\n",
    "The entire conversation with the assistant, with the user's original request and all replies, is:\n",
    "{format_conversation(state['messages'])}\n",
    "\n",
    "The success criteria for this assignment is:\n",
    "{state['success_criteria']}\n",
    "\n",
    "And the final response from the Assistant that you are evaluating is:\n",
    "{last_response}\n",
    "\n",
    "Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.\n",
    "\"\"\"\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "    \n",
    "    evaluator_messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "\n",
    "    eval_result = evaluator_llm_with_output.invoke(evaluator_messages)\n",
    "    new_state = {\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": f\"Evaluator Feedback on this answer: {eval_result.feedback}\"}],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"user_input_needed\": eval_result.user_input_needed\n",
    "    }\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"worker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac09554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Graph Builder with State\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"worker\", worker)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_conditional_edges(\"worker\", worker_router, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"worker\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", route_based_on_evaluation, {\"worker\": \"worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"worker\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2393b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread}}\n",
    "\n",
    "    state = {\n",
    "        \"messages\": message,\n",
    "        \"success_criteria\": success_criteria,\n",
    "        \"feedback_on_work\": None,\n",
    "        \"success_criteria_met\": False,\n",
    "        \"user_input_needed\": False\n",
    "    }\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "    user = {\"role\": \"user\", \"content\": message}\n",
    "    reply = {\"role\": \"assistant\", \"content\": result[\"messages\"][-2].content}\n",
    "    feedback = {\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}\n",
    "    return history + [user, reply, feedback]\n",
    "\n",
    "async def reset():\n",
    "    return \"\", \"\", None, make_thread_id()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "    gr.Markdown(\"## Sidekick Personal Co-worker\")\n",
    "    thread = gr.State(make_thread_id())\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Sidekick\", height=300, type=\"messages\")\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success critiera?\")\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "\n",
    "    \n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
